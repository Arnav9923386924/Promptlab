<p align="center">
  <h1 align="center">üß™ PromptLab</h1>
</p>

<p align="center">
  <strong>CI/CD for LLM Applications</strong><br>
  Test prompts at every pipeline stage. Catch regressions before production.<br>
  Evaluate with an LLM Council, not a single biased judge.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.10+-blue.svg" alt="Python 3.10+">
  <img src="https://img.shields.io/badge/council-evaluation-gold.svg" alt="Council">
  <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="MIT">
</p>

---

## ‚ö° Quick Start (60 seconds)

### 1. Install

```bash
pip install promptlab
```

**Or install from source:**
```bash
git clone https://github.com/Arnav9923386924/Promptlab.git
cd promptlab
pip install -e .
```

### 2. Initialize Project

```bash
cd your-project
promptlab init
```

This creates:
- `promptlab.yaml` - Configuration file
- `temp/` - Test files directory
- `.promptlab/` - Baselines and run history

### 3. Setup Provider

**Using Ollama (Local, Free):**
```bash
promptlab setup ollama
```

**Using OpenRouter (Cloud):**
```bash
promptlab setup openrouter --api-key YOUR_KEY
```

### 4. Setup GitHub Actions (Auto-Integration)

```bash
promptlab ci-setup
```

This automatically creates `.github/workflows/prompt-tests.yml` with:
- Test execution on PRs
- BSP validation on main branch
- Automatic baseline updates

**Enable auto-push when BSP improves:**
```bash
promptlab ci-setup --auto-push
```

Then commit and push:
```bash
git add .github/workflows/prompt-tests.yml
git commit -m "ci: add PromptLab workflow"
git push
```

‚úÖ **That's it! Your CI/CD is now active.**

### 5. Run Tests

**Test with existing YAML files:**
```bash
promptlab run roleplay
```

**Dynamic test generation (auto-creates tests for any role):**
```bash
promptlab run roleplay --role "You are a Python expert"
```

**Run HuggingFace benchmarks:**
```bash
promptlab run performance
```

**Validate your BSP:**
```bash
promptlab validate
```

---

## üéØ What Problem Does This Solve?

You're building an LLM-powered app. You have prompts in production. You make changes.

**The Problem:**
- How do you know your changes didn't break something?
- How do you test each stage of your LLM pipeline?
- How do you get unbiased evaluation of LLM outputs?

**PromptLab is the answer:** Automated testing for LLM applications with council-based evaluation.

---

## üî• Key Features

| Feature | Description |
|---------|-------------|
| **Dynamic Test Generation** | `--role "You are X"` ‚Üí Auto-generates relevant tests |
| **Web Scraping** | Dynamic benchmark generation from any topic with production-grade stack |
| **LLM Council** | Multiple models evaluate responses, reach consensus |
| **HuggingFace Benchmarks** | Import MMLU, GSM8K, TruthfulQA with one command |
| **Parallel Execution** | Run tests concurrently for speed |
| **OpenRouter + Ollama** | Use local or cloud models seamlessly |

---

## üìÅ Project Structure

```
promptlab/
‚îú‚îÄ‚îÄ promptlab.yaml          # Your config (API keys, models)
‚îú‚îÄ‚îÄ promptlab.example.yaml  # Example config to copy
‚îú‚îÄ‚îÄ tests/                  # Test YAML files
‚îÇ   ‚îú‚îÄ‚îÄ sentiment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ code_review.yaml
‚îÇ   ‚îú‚îÄ‚îÄ council_test.yaml   # Tests using LLM Council
‚îÇ   ‚îî‚îÄ‚îÄ generated_*.yaml    # Auto-generated tests
‚îî‚îÄ‚îÄ src/                    # Source code
```

---

## ‚öôÔ∏è Configuration

Copy `promptlab.example.yaml` to `promptlab.yaml` and customize:

```yaml
# promptlab.yaml
version: 1

models:
  default: ollama/llama3.1:8b           # Model to test
  generator: ollama/llama3.1:8b         # Model for generating tests
  
  providers:
    ollama:
      endpoint: http://localhost:11434
    openrouter:
      api_key: your-openrouter-api-key

council:
  enabled: true
  mode: fast  # full | fast | vote
  members:
    - ollama/llama3.1:8b
    - ollama/llama3.2:3b
  chairman: ollama/llama3.1:8b

testing:
  parallelism: 4
  timeout_ms: 30000

scraper:
  # Brave Search API (OPTIONAL - requires credit card even for free tier)
  brave_api_key: ""  # Leave empty to use free alternatives
  
  # Free search providers (no auth required)
  fallback_search: searxng  # searxng | duckduckgo | google_scrape
  max_pages: 5
  timeout: 30
```

---

## üèõÔ∏è LLM Council (Inspired by Karpathy)

Single LLM-as-judge = biased toward its own style.

**LLM Council = multiple models deliberate, cross-critique, reach consensus.**

```
STAGE 1: Independent Judging
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Llama  ‚îÇ  ‚îÇ Gemma  ‚îÇ  ‚îÇ Mistral‚îÇ
‚îÇ 0.82   ‚îÇ  ‚îÇ 0.75   ‚îÇ  ‚îÇ 0.78   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
STAGE 2: Cross-Critique (Optional)
"Judge A's score seems high because..."
                ‚îÇ
STAGE 3: Chairman Synthesis
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Final Score: 0.78                   ‚îÇ
‚îÇ Confidence: HIGH                    ‚îÇ
‚îÇ Consensus: "Accurate but verbose"   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Council Test Example:

```yaml
# tests/council_test.yaml
cases:
  - id: quality-check
    prompt: "Explain Python decorators in 2 sentences."
    assertions:
      - type: council_judge
        criteria: "Response correctly explains decorators"
        min_score: 0.7
```

---

## üìä Free OpenRouter Models

For users without paid API access:

| Model ID | Provider |
|----------|----------|
| `openrouter/meta-llama/llama-3.1-8b-instruct:free` | Meta |
| `openrouter/google/gemma-2-9b-it:free` | Google |
| `openrouter/mistralai/mistral-7b-instruct:free` | Mistral |

Use in config:
```yaml
generator: openrouter/google/gemma-2-9b-it:free
```

---

## üìã Commands Reference

| Command | Description |
|---------|-------------|
| `promptlab init` | Initialize project with example config |
| `promptlab setup ollama` | Quick setup for Ollama |
| `promptlab setup openrouter -k KEY` | Quick setup for OpenRouter |
| `promptlab test` | Run all YAML tests |
| `promptlab test tests/file.yaml` | Run specific test file |
| `promptlab run roleplay` | Run existing roleplay tests |
| `promptlab run roleplay --role "..."` | Generate + run tests for role |
| `promptlab run performance` | Run HuggingFace benchmarks |
| `promptlab benchmark gsm8k` | Download GSM8K benchmark |
| `promptlab scrape "topic" --pages N` | Scrape web to generate benchmarks dynamically |
| `promptlab validate` | Validate BSP and compare with baseline |
| `promptlab validate --push` | Validate and push to git if improved |
| `promptlab validate --ci` | CI mode for GitHub Actions |

---

## üîç BSP Validation (Behavior Specification Prompt)

Validate your LLM's behavior specification prompt with council-based evaluation.

### What is BSP Validation?

Your **Behavior Specification Prompt (BSP)** is the system prompt that defines how your LLM should behave. BSP validation:

1. **Prepends your BSP** to all test prompts
2. **Collects all outputs** into a single file
3. **Submits to LLM Council** for batch evaluation
4. **Compares scores** with baseline
5. **Auto-pushes to git** if score improves (optional)

### Configuration:

```yaml
# promptlab.yaml
bsp:
  prompt: |
    You are a helpful assistant.
    Always be accurate and concise.
    If unsure, say so.
  min_score: 0.7
  version: "1.0.0"

baseline:
  auto_update: true
  min_improvement: 0.01

git:
  enabled: true
  commit_template: "chore: BSP validation (score: {score:.2f})"
  auto_push: false
```

### Usage:

```bash
# Basic validation
promptlab validate

# Validate and push to git if improved
promptlab validate --push

# CI mode (exit code based on result)
promptlab validate --ci

# Save JSON output
promptlab validate --output result.json
```

### CI/CD Integration (GitHub Actions):

```yaml
# .github/workflows/prompt-tests.yml
jobs:
  bsp-validation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
      - run: pip install promptlab
      - run: promptlab validate --ci
```

### How It Works:

```
1. Load BSP from promptlab.yaml
   ‚Üì
2. Run all tests with BSP prepended
   ‚Üì
3. Collect outputs to .promptlab/runs/
   ‚Üì
4. Submit batch to LLM Council
   ‚Üì
5. Council evaluates:
   - Role Adherence (0-1)
   - Response Quality (0-1)
   - Consistency (0-1)
   - Appropriateness (0-1)
   ‚Üì
6. Compare with baseline
   ‚Üì
7. If improved ‚Üí Update baseline ‚Üí Push to git
```

---

## üöÄ CI/CD Auto-Integration

PromptLab automatically sets up GitHub Actions with one command.

### Quick Setup:

```bash
# Navigate to your project
cd your-project

# Generate GitHub Actions workflow
promptlab ci-setup

# Commit and push
git add .github/workflows/prompt-tests.yml
git commit -m "ci: add PromptLab workflow"
git push
```

‚úÖ **Done! Your CI/CD is now active.**

### Advanced Options:

```bash
# Enable auto-push when BSP score improves
promptlab ci-setup --auto-push

# Skip BSP validation job (only run basic tests)
promptlab ci-setup --no-bsp

# Customize later by editing .github/workflows/prompt-tests.yml
```

### What Gets Created:

The `ci-setup` command generates a complete GitHub Actions workflow that:

| Feature | Description |
|---------|-------------|
| **Test on PR** | Runs `promptlab test --ci` on every PR |
| **BSP Validation** | Validates BSP on main branch pushes |
| **Baseline Updates** | Automatically updates baselines when scores improve |
| **Artifact Upload** | Saves validation results as artifacts |
| **Auto-Push** | (Optional) Commits and pushes improved baselines |
| **Ollama Installation** | Automatically installs and configures Ollama |
| **PromptLab from PyPI** | Installs latest PromptLab via `pip install promptlab` |

### Workflow Structure:

```yaml
# .github/workflows/prompt-tests.yml (auto-generated)
name: Prompt Tests

on:
  pull_request:
    paths: ['prompts/**', 'tests/**', 'promptlab.yaml']
  push:
    branches: [main]

jobs:
  prompt-tests:         # Run on every PR
    - pip install promptlab
    - promptlab test --ci
  
  bsp-validation:       # Run on main branch only
    - promptlab validate --ci
    - Upload artifacts
    - Auto-push if improved (optional)
```

### Using OpenRouter Instead of Ollama:

If you prefer OpenRouter (cloud models), edit the workflow after generation:

```yaml
# Replace Ollama installation with:
- name: Configure OpenRouter
  env:
    OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
  run: |
    echo "OPENROUTER_API_KEY=${OPENROUTER_API_KEY}" >> $GITHUB_ENV
```

Add your API key to GitHub Secrets:
1. Go to Settings ‚Üí Secrets ‚Üí Actions
2. Add `OPENROUTER_API_KEY`

---

## üéöÔ∏è Council Modes

| Mode | Stages | Speed | Use Case |
|------|--------|-------|----------|
| `full` | All 3 | Slow | Critical tests |
| `fast` | 2 (skip critique) | Medium | Regular testing |
| `vote` | Just majority | Fast | Quick sanity checks |

---

## üåê Web Scraping (Dynamic Benchmark Generation)

Generate custom benchmarks from **any topic** by scraping the web - no static datasets required!

### Quick Example:

```bash
# Scrape 5 pages about quantum computing
promplab scrape "quantum computing" --pages 5 --output benchmark

# Run the generated tests
promplab test tests/benchmark.yaml
```

### Tech Stack (Production-Grade):

| Component | Technology | Purpose |
|-----------|------------|----------|
| **HTTP Client** | `httpx` (async, HTTP/2) | Fast request-based scraping |
| **Browser** | `Playwright` + stealth | JS-rendered pages, bot protection bypass |
| **HTML Parser** | `selectolax` | 5-10x faster than BeautifulSoup |
| **Search** | SearXNG (free) | Dynamic URL discovery, no auth needed |

### Hybrid Approach:

```
1. Try httpx (fast, lightweight)
   ‚Üì
2. Detect if page needs JavaScript
   ‚Üì
3. Fallback to Playwright (stealth mode)
   ‚Üì
4. Extract Q&A pairs automatically
   ‚Üì
5. Generate YAML test suite
```

### Search Providers:

| Provider | Auth Required | Rate Limits | Quality |
|----------|---------------|-------------|----------|
| **SerpAPI** (recommended) | ‚úÖ Free API key | 100/month free | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **SearXNG** (default fallback) | ‚ùå No | ‚úÖ None | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Brave Search API** | ‚ö†Ô∏è Credit card | 2,000/month | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **DuckDuckGo** | ‚ùå No | ‚ö†Ô∏è Sometimes | ‚≠ê‚≠ê‚≠ê |
| **Google Scrape** | ‚ùå No | ‚ö†Ô∏è Often blocked | ‚≠ê‚≠ê |

**Recommendation:** Get a free SerpAPI key at https://serpapi.com/ (no credit card required!) for best Google results. Or leave it empty to use SearXNG.

### Usage:

```bash
# Basic scraping
promplab scrape "machine learning"

# Specify page count
promplab scrape "python decorators" --pages 3

# Custom output file
promplab scrape "quantum entanglement" --output quantum_tests

# Auto-run tests after scraping
promplab scrape "redis caching" --run
```

### Generated Output:

```yaml
# tests/benchmark.yaml
metadata:
  name: "Web-Scraped Test Suite: quantum computing"
  description: "Auto-generated from 5 web sources"
  
cases:
  - id: qa-1
    prompt: "What is quantum superposition?"
    assertions:
      - type: llm_judge
        criteria: "Explains quantum states existing in multiple configurations"
        
  - id: qa-2
    prompt: "How does quantum entanglement work?"
    assertions:
      - type: council_judge
        criteria: "Describes particle correlation across distance"
```

---

## üìä HuggingFace Benchmarks

Download and test with standard LLM benchmarks:

```bash
# Download benchmark
promptlab benchmark gsm8k --samples 20

# Run performance test
promptlab run performance
```

**Available benchmarks:** `gsm8k`, `mmlu`, `truthfulqa`, `hellaswag`

---

## üîÑ CI/CD Integration

```yaml
# .github/workflows/prompt-tests.yml
name: Prompt Tests

on:
  pull_request:
    paths: ['tests/**', 'promptlab.yaml']

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install promptlab
      - run: |
          curl -fsSL https://ollama.ai/install.sh | sh
          ollama serve &
          sleep 5
          ollama pull llama3.1:8b
      - run: promptlab test --ci
```

---

## üöÄ Roadmap

- [x] Core framework + Ollama integration
- [x] Council evaluation engine
- [x] Dynamic test generation
- [x] HuggingFace benchmark import
- [x] Parallel test execution
- [ ] VS Code extension
- [ ] Production log capture

---

## üì¶ Publishing to PyPI (For Maintainers)

```bash
# Install build tools
pip install build twine

# Build the package
python -m build

# Upload to PyPI
python -m twine upload dist/*
```

Users can then install with:
```bash
pip install promptlab
```

---

## ü§ù Contributing

PRs welcome! See [CONTRIBUTING.md](CONTRIBUTING.md)

---

## üìÑ License

MIT

---

<p align="center">
  <b>Stop testing prompts manually. Start shipping with confidence.</b>
</p>
