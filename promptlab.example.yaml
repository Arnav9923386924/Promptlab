# PromptLab Example Configuration
# Copy this to promptlab.yaml and customize for your setup
version: 1

models:
  # The LLM to test
  default: ollama/llama3.1:8b
  
  # LLM for generating test cases (optional, falls back to default)
  generator: ollama/llama3.1:8b
  
  providers:
    # Local Ollama (Free)
    ollama:
      endpoint: http://localhost:11434
    
    # OpenRouter - Access many models with one key
    # openrouter:
    #   api_key: sk-or-xxx
    
    # OpenAI Direct
    # openai:
    #   api_key: sk-xxx
    
    # Anthropic Claude Direct
    # anthropic:
    #   api_key: sk-ant-xxx
    
    # Google Gemini Direct
    # google:
    #   api_key: AIza-xxx
    
    # xAI Grok Direct
    # xai:
    #   api_key: xai-xxx

# LLM Council for multi-model evaluation
council:
  enabled: true
  mode: fast  # Options: full | fast | vote
  members:
    - ollama/llama3.1:8b
    - ollama/llama3.2:3b  # Use any models you have installed
  chairman: ollama/llama3.1:8b  # Makes final decision

# Test execution settings
testing:
  parallelism: 4      # Number of parallel tests
  timeout_ms: 30000   # Test timeout in milliseconds

# Example model identifiers for each provider:
# 
# ollama:
#   - ollama/llama3.1:8b
#   - ollama/llama3.2:3b
#   - ollama/mistral:7b
#
# openrouter:
#   - openrouter/openai/gpt-4o
#   - openrouter/anthropic/claude-3.5-sonnet
#   - openrouter/google/gemini-2.0-flash-001
#   - openrouter/meta-llama/llama-3.1-8b-instruct:free
#
# openai:
#   - openai/gpt-4o
#   - openai/gpt-4o-mini
#   - openai/gpt-3.5-turbo
#
# anthropic:
#   - anthropic/claude-3-5-sonnet-20241022
#   - anthropic/claude-3-haiku-20240307
#
# google:
#   - google/gemini-2.0-flash
#   - google/gemini-1.5-pro
#
# xai:
#   - xai/grok-2
#   - xai/grok-beta
